# Document Intelligence Platform

A production-grade RAG (Retrieval Augmented Generation) system built with experiment-driven development.

## ğŸ¯ Project Philosophy

This isn't just another RAG tutorial. This project demonstrates **engineering judgment** through:

- **Measured improvements**: Every feature is evaluated against baseline metrics
- **Documented trade-offs**: Design decisions explained with data
- **Intentional removals**: Features that didn't help were removed and documented
- **Production practices**: Proper configuration, testing, and observability

## ğŸ“Š Performance Journey

| Version | Change | Recall@5 | MRR | Latency | Decision |
|---------|--------|----------|-----|---------|----------|
| v0.1 | Baseline (fixed chunking, semantic search) | TBD | TBD | TBD | Baseline |

*Table updated as experiments are completed*

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    RAG Pipeline v0.1                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  INGEST          RETRIEVE         GENERATE                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚ Loader  â”‚ â†’  â”‚ Embed   â”‚  â†’   â”‚ Context â”‚               â”‚
â”‚  â”‚ Chunker â”‚    â”‚ Search  â”‚      â”‚ + LLM   â”‚               â”‚
â”‚  â”‚ Embed   â”‚    â”‚         â”‚      â”‚         â”‚               â”‚
â”‚  â”‚ Store   â”‚    â”‚         â”‚      â”‚         â”‚               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚       â”‚              â”‚                                      â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                      â”‚
â”‚              â”‚                                              â”‚
â”‚       â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”                                       â”‚
â”‚       â”‚  ChromaDB   â”‚                                       â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                       â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

```bash
# Clone and install
git clone https://github.com/your-username/doc-intelligence.git
cd doc-intelligence
pip install -e .

# Set API key
export OPENAI_API_KEY=your_key

# Run test
python -m tests.test_baseline
```

## ğŸ“ Project Structure

```
doc-intelligence/
â”œâ”€â”€ src/docint/
â”‚   â”œâ”€â”€ config.py           # Typed configuration
â”‚   â”œâ”€â”€ pipeline.py         # Main RAG pipeline
â”‚   â”œâ”€â”€ ingest/             # Document loading & chunking
â”‚   â”œâ”€â”€ embeddings/         # Embedding providers
â”‚   â”œâ”€â”€ store/              # Vector store implementations
â”‚   â”œâ”€â”€ retrieval/          # Retrieval strategies
â”‚   â”œâ”€â”€ generation/         # LLM generation
â”‚   â””â”€â”€ evaluation/         # Metrics & evaluation
â”œâ”€â”€ experiments/            # Documented experiments
â”‚   â””â”€â”€ 001_baseline/       # Baseline metrics
â”œâ”€â”€ configs/                # YAML configurations
â”œâ”€â”€ eval_data/              # Test documents & Q&A pairs
â””â”€â”€ tests/                  # Test suite
```

## ğŸ”¬ Experiments

Each experiment follows a rigorous process:
1. **Hypothesis**: What we expect to improve
2. **Implementation**: Code changes
3. **Measurement**: Metrics before/after
4. **Decision**: Keep, modify, or remove

See [`experiments/`](experiments/) for full documentation.

## ğŸ“ˆ Evaluation Metrics

### Retrieval Metrics
- **Recall@K**: Did we find all relevant documents?
- **Precision@K**: How much noise in results?
- **MRR**: Where does first relevant doc appear?
- **NDCG@K**: How well are relevant docs ranked?

### Generation Metrics (planned)
- **Faithfulness**: Is answer grounded in context?
- **Relevance**: Does answer address the question?

## ğŸ› ï¸ Configuration

```yaml
# configs/default.yaml
chunking:
  strategy: fixed      # fixed | recursive | semantic
  chunk_size: 512
  chunk_overlap: 50

embedding:
  model: text-embedding-3-small

retrieval:
  top_k: 5

generation:
  model: gpt-4o-mini
```

## ğŸ“ Design Decisions

### Why Fixed Chunking for Baseline?
Simple and predictable. Establishes clear baseline before trying complex strategies.
We'll compare against recursive and semantic chunking in Experiment 002.

### Why ChromaDB?
- Embedded (no separate server)
- Good for development and small-to-medium scale
- Easy to swap out later if needed

### Why OpenAI Embeddings?
- High quality out of the box
- Consistent API
- Will compare against local embeddings for cost/latency trade-off

## ğŸ¯ Roadmap

- [x] v0.1: Baseline RAG pipeline
- [ ] v0.2: Chunking strategy comparison
- [ ] v0.3: Hybrid search (BM25 + semantic)
- [ ] v0.4: Cross-encoder reranking
- [ ] v0.5: HyDE query expansion
- [ ] v0.6: Hallucination detection
- [ ] v1.0: Production-ready with full evaluation

## ğŸ“„ License

MIT
